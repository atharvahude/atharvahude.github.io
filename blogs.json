{"status":"ok","feed":{"url":"https://medium.com/feed/@atharvahude005","title":"Stories by Atharva Hude on Medium","link":"https://medium.com/@atharvahude005?source=rss-d2a88993f4cd------2","author":"","description":"Stories by Atharva Hude on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*fo2OY1y7IEzG8Xg6.jpg"},"items":[{"title":"Investigating Data Leakage in Vision-Language Models (VLMs)","pubDate":"2025-02-28 00:19:50","link":"https://medium.com/@atharvahude005/investigating-data-leakage-in-vision-language-models-vlms-a3c79c0994e7?source=rss-d2a88993f4cd------2","guid":"https://medium.com/p/a3c79c0994e7","author":"Atharva Hude","thumbnail":"","description":"\n<h3>Introduction</h3>\n<p>With the rise of Vision-Language Models (VLMs), their impressive performance on various benchmarks has sparked debate over whether they genuinely understand and solve tasks or merely retrieve stored knowledge. This concern arises due to the secrecy surrounding the training datasets used by major tech companies. If test and training datasets overlap, it could artificially inflate model performance, misleading researchers and practitioners.</p>\n<p>Our project sought to explore potential data leakage in VLMs by systematically testing whether models had prior exposure to test set samples. By implementing various perturbations and evaluation metrics, we examined multiple models to assess their robustness and fairness.</p>\n<h3>Approach</h3>\n<p>We devised a series of tests targeting different aspects of model knowledge retrieval to detect data leakage. Our methodology included:</p>\n<ol>\n<li>\n<strong>Question + Answer Reconstruction</strong>\u200a\u2014\u200aPrompting models with partially completed question-answer sequences and evaluating their ability to reconstruct them.</li>\n<li>\n<strong>Question Completion</strong>\u200a\u2014\u200aRemoving critical tokens or restructuring questions to determine whether the model could accurately recover missing\u00a0parts.</li>\n<li>\n<strong>Checking Relevant Information</strong>\u200a\u2014\u200aTesting whether models had prior knowledge of specific dataset concepts that could aid in answering questions.</li>\n<li>\n<strong>Paraphrasing &amp; Restructuring</strong>\u200a\u2014\u200aAltering question phrasing and structure to evaluate whether models retained memory of specific questions.</li>\n<li>\n<strong>Image Manipulation</strong>\u200a\u2014\u200aFlipping, rotating, or adding distortions to images to check if models relied on memorized images.</li>\n<li>\n<strong>Bias Assessment</strong>\u200a\u2014\u200aChecking whether models displayed biases when answering questions with missing or altered contextual information.</li>\n</ol>\n<h3>Methodology &amp; Models\u00a0Tested</h3>\n<p>We tested multiple VLMs across various datasets, using different evaluation strategies to determine the extent of data\u00a0leakage:</p>\n<ul>\n<li>\n<strong>Llava-1.5\u20137B</strong> (VQA v2 dataset, 50\u00a0samples)</li>\n<li>\n<strong>Qwen-7B-VL</strong> (ScienceQA dataset, 200\u00a0samples)</li>\n<li>\n<strong>Microsoft Phi-3-Vision-128k-Instruct</strong> (MathVista dataset, 418\u00a0samples)</li>\n<li>\n<strong>Llama 3.2 11B Vision Instruct &amp; pretrained</strong> (ScienceQA &amp; ChartQA datasets, 30+\u00a0samples)</li>\n</ul>\n<p>For evaluation, we measured model accuracy under different perturbation conditions and used <strong>Rouge metrics</strong> for overlap analysis.</p>\n<h3>Key Findings</h3>\n<h4>Qwen-7B-VL (ScienceQA Test\u00a0Split)</h4>\n<ul>\n<li>Baseline accuracy (original image + question) was\u00a0<strong>76.29%</strong>.</li>\n<li>Removing images significantly impacted performance (<strong>52.58%</strong>).</li>\n<li>Truncating questions reduced accuracy to <strong>47.50%</strong>, suggesting dependency on explicit phrasing.</li>\n<li>Flipping or rotating images altered model responses, indicating potential memorization of image positions.</li>\n</ul>\n<h4>Llava-1.5\u20137B (VQA v2 Val\u00a0Dataset)</h4>\n<ul>\n<li>Performance remained consistent across various transformations, implying minimal data\u00a0leakage.</li>\n<li>Surprisingly, accuracy <strong>increased</strong> for certain image rotations, an anomaly warranting further exploration.</li>\n</ul>\n<h4>Microsoft Phi-3-Vision-128k-Instruct (MathVista)</h4>\n<ul>\n<li>Removing images led to a drastic accuracy drop (<strong>8.13%</strong>), showing a strong reliance on visual\u00a0context.</li>\n<li>Using paraphrased questions slightly reduced accuracy, suggesting potential memorization effects.</li>\n</ul>\n<h4>Llama 3.2 11B Vision Models (ScienceQA &amp;\u00a0ChartQA)</h4>\n<ul>\n<li>The <strong>pre-trained model</strong> performed poorly compared to the <strong>fine-tuned instruct\u00a0model</strong>.</li>\n<li>Removing key question components did not always decrease accuracy, hinting at potential memorization of test set distributions.</li>\n<li>The model sometimes produced additional metadata (e.g., required skills) unprompted, suggesting prior exposure to similar test distributions.</li>\n</ul>\n<h3>Implications &amp; Future\u00a0Work</h3>\n<p>Our findings suggest that while some VLMs exhibit minimal data leakage, others show signs of potential overfitting to training datasets. This raises concerns about fairness and generalization in real-world applications. Moving forward, we recommend:</p>\n<ol>\n<li>\n<strong>Stricter Evaluation Protocols</strong>\u200a\u2014\u200aImplementing robust benchmarks that ensure no overlap between training and test\u00a0sets.</li>\n<li>\n<strong>Dataset Transparency</strong>\u200a\u2014\u200aEncouraging more openness from model developers regarding pretraining data.</li>\n<li>\n<strong>Adversarial Testing</strong>\u200a\u2014\u200aUsing targeted adversarial samples to detect hidden memorization biases.</li>\n</ol>\n<p>As VLMs continue to evolve, ensuring fair and unbiased evaluation will be crucial in developing trustworthy AI systems. Our research highlights the importance of rigorous testing and the need for greater transparency in AI model development.</p>\n<h3>References</h3>\n<p>For further reading, check out our detailed analysis and methodology on <a href=\"https://github.com/atharvahude/VLM-Data-Leakage-Study\">GitHub</a>. Additional relevant studies\u00a0include:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2304.08485\">https://arxiv.org/abs/2304.08485</a></li>\n<li><a href=\"https://arxiv.org/pdf/2404.18824\">https://arxiv.org/pdf/2404.18824</a></li>\n<li><a href=\"https://aclanthology.org/2024.naacl-long.424.pdf\">https://aclanthology.org/2024.naacl-long.424.pdf</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3c79c0994e7\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Introduction</h3>\n<p>With the rise of Vision-Language Models (VLMs), their impressive performance on various benchmarks has sparked debate over whether they genuinely understand and solve tasks or merely retrieve stored knowledge. This concern arises due to the secrecy surrounding the training datasets used by major tech companies. If test and training datasets overlap, it could artificially inflate model performance, misleading researchers and practitioners.</p>\n<p>Our project sought to explore potential data leakage in VLMs by systematically testing whether models had prior exposure to test set samples. By implementing various perturbations and evaluation metrics, we examined multiple models to assess their robustness and fairness.</p>\n<h3>Approach</h3>\n<p>We devised a series of tests targeting different aspects of model knowledge retrieval to detect data leakage. Our methodology included:</p>\n<ol>\n<li>\n<strong>Question + Answer Reconstruction</strong>\u200a\u2014\u200aPrompting models with partially completed question-answer sequences and evaluating their ability to reconstruct them.</li>\n<li>\n<strong>Question Completion</strong>\u200a\u2014\u200aRemoving critical tokens or restructuring questions to determine whether the model could accurately recover missing\u00a0parts.</li>\n<li>\n<strong>Checking Relevant Information</strong>\u200a\u2014\u200aTesting whether models had prior knowledge of specific dataset concepts that could aid in answering questions.</li>\n<li>\n<strong>Paraphrasing &amp; Restructuring</strong>\u200a\u2014\u200aAltering question phrasing and structure to evaluate whether models retained memory of specific questions.</li>\n<li>\n<strong>Image Manipulation</strong>\u200a\u2014\u200aFlipping, rotating, or adding distortions to images to check if models relied on memorized images.</li>\n<li>\n<strong>Bias Assessment</strong>\u200a\u2014\u200aChecking whether models displayed biases when answering questions with missing or altered contextual information.</li>\n</ol>\n<h3>Methodology &amp; Models\u00a0Tested</h3>\n<p>We tested multiple VLMs across various datasets, using different evaluation strategies to determine the extent of data\u00a0leakage:</p>\n<ul>\n<li>\n<strong>Llava-1.5\u20137B</strong> (VQA v2 dataset, 50\u00a0samples)</li>\n<li>\n<strong>Qwen-7B-VL</strong> (ScienceQA dataset, 200\u00a0samples)</li>\n<li>\n<strong>Microsoft Phi-3-Vision-128k-Instruct</strong> (MathVista dataset, 418\u00a0samples)</li>\n<li>\n<strong>Llama 3.2 11B Vision Instruct &amp; pretrained</strong> (ScienceQA &amp; ChartQA datasets, 30+\u00a0samples)</li>\n</ul>\n<p>For evaluation, we measured model accuracy under different perturbation conditions and used <strong>Rouge metrics</strong> for overlap analysis.</p>\n<h3>Key Findings</h3>\n<h4>Qwen-7B-VL (ScienceQA Test\u00a0Split)</h4>\n<ul>\n<li>Baseline accuracy (original image + question) was\u00a0<strong>76.29%</strong>.</li>\n<li>Removing images significantly impacted performance (<strong>52.58%</strong>).</li>\n<li>Truncating questions reduced accuracy to <strong>47.50%</strong>, suggesting dependency on explicit phrasing.</li>\n<li>Flipping or rotating images altered model responses, indicating potential memorization of image positions.</li>\n</ul>\n<h4>Llava-1.5\u20137B (VQA v2 Val\u00a0Dataset)</h4>\n<ul>\n<li>Performance remained consistent across various transformations, implying minimal data\u00a0leakage.</li>\n<li>Surprisingly, accuracy <strong>increased</strong> for certain image rotations, an anomaly warranting further exploration.</li>\n</ul>\n<h4>Microsoft Phi-3-Vision-128k-Instruct (MathVista)</h4>\n<ul>\n<li>Removing images led to a drastic accuracy drop (<strong>8.13%</strong>), showing a strong reliance on visual\u00a0context.</li>\n<li>Using paraphrased questions slightly reduced accuracy, suggesting potential memorization effects.</li>\n</ul>\n<h4>Llama 3.2 11B Vision Models (ScienceQA &amp;\u00a0ChartQA)</h4>\n<ul>\n<li>The <strong>pre-trained model</strong> performed poorly compared to the <strong>fine-tuned instruct\u00a0model</strong>.</li>\n<li>Removing key question components did not always decrease accuracy, hinting at potential memorization of test set distributions.</li>\n<li>The model sometimes produced additional metadata (e.g., required skills) unprompted, suggesting prior exposure to similar test distributions.</li>\n</ul>\n<h3>Implications &amp; Future\u00a0Work</h3>\n<p>Our findings suggest that while some VLMs exhibit minimal data leakage, others show signs of potential overfitting to training datasets. This raises concerns about fairness and generalization in real-world applications. Moving forward, we recommend:</p>\n<ol>\n<li>\n<strong>Stricter Evaluation Protocols</strong>\u200a\u2014\u200aImplementing robust benchmarks that ensure no overlap between training and test\u00a0sets.</li>\n<li>\n<strong>Dataset Transparency</strong>\u200a\u2014\u200aEncouraging more openness from model developers regarding pretraining data.</li>\n<li>\n<strong>Adversarial Testing</strong>\u200a\u2014\u200aUsing targeted adversarial samples to detect hidden memorization biases.</li>\n</ol>\n<p>As VLMs continue to evolve, ensuring fair and unbiased evaluation will be crucial in developing trustworthy AI systems. Our research highlights the importance of rigorous testing and the need for greater transparency in AI model development.</p>\n<h3>References</h3>\n<p>For further reading, check out our detailed analysis and methodology on <a href=\"https://github.com/atharvahude/VLM-Data-Leakage-Study\">GitHub</a>. Additional relevant studies\u00a0include:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2304.08485\">https://arxiv.org/abs/2304.08485</a></li>\n<li><a href=\"https://arxiv.org/pdf/2404.18824\">https://arxiv.org/pdf/2404.18824</a></li>\n<li><a href=\"https://aclanthology.org/2024.naacl-long.424.pdf\">https://aclanthology.org/2024.naacl-long.424.pdf</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3c79c0994e7\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["llm","vlm","ai"]},{"title":"Install VS Code Jetson Orin","pubDate":"2024-05-12 06:00:54","link":"https://medium.com/@atharvahude005/install-vs-code-jetson-orin-fefcd80a471e?source=rss-d2a88993f4cd------2","guid":"https://medium.com/p/fefcd80a471e","author":"Atharva Hude","thumbnail":"","description":"\n<p>Title: How to Install Visual Studio Code on Nvidia Jetson Orin: A Step-by-Step Guide</p>\n<p>Introduction:<br>Nvidia Jetson Orin is a powerful platform that can be used for a variety of tasks, including AI and robotics development. One of the essential tools for software development is a code editor, and <strong>Visual Studio Code (VS Code)</strong> stands out as a popular choice among developers due to its versatility and extensive ecosystem of extensions. In this tutorial, we\u2019ll walk you through the process of installing VS Code on Nvidia Jetson\u00a0Orin.</p>\n<p><strong>Step 1:</strong> Download the ARM64 Version of Visual Studio Code<br><a href=\"https://code.visualstudio.com/download#\">https://code.visualstudio.com/download</a></p>\n<p><strong>Step 2: </strong>Install Visual Studio Code<br>Once you have downloaded the\u00a0.deb package of Visual Studio Code, you can proceed with the installation process. Open a terminal window on your Nvidia Jetson Orin device and navigate to the directory where the\u00a0.deb package is\u00a0located.</p>\n<p>Use the following command to install the package:<br><br><strong>sudo dpkg -i package_file.deb</strong><br><br>Replace \u201cpackage_file.deb\u201d with the name of the\u00a0.deb package you downloaded. This command will install Visual Studio Code on your Nvidia Jetson Orin\u00a0device.</p>\n<p><strong>Step 3:</strong> Launch Visual Studio Code<br>After the installation process is complete, you can launch Visual Studio Code from the application menu or by typing \u201ccode\u201d in the terminal. Once launched, you\u2019ll have access to a powerful and feature-rich code editor that you can use for your software development projects.</p>\n<p><strong>Conclusion:</strong><br>In this tutorial, we\u2019ve covered the steps required to install Visual Studio Code on Nvidia Jetson Orin. By following these steps, you can set up a powerful development environment for your projects and take advantage of the features offered by Visual Studio\u00a0Code.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fefcd80a471e\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Title: How to Install Visual Studio Code on Nvidia Jetson Orin: A Step-by-Step Guide</p>\n<p>Introduction:<br>Nvidia Jetson Orin is a powerful platform that can be used for a variety of tasks, including AI and robotics development. One of the essential tools for software development is a code editor, and <strong>Visual Studio Code (VS Code)</strong> stands out as a popular choice among developers due to its versatility and extensive ecosystem of extensions. In this tutorial, we\u2019ll walk you through the process of installing VS Code on Nvidia Jetson\u00a0Orin.</p>\n<p><strong>Step 1:</strong> Download the ARM64 Version of Visual Studio Code<br><a href=\"https://code.visualstudio.com/download#\">https://code.visualstudio.com/download</a></p>\n<p><strong>Step 2: </strong>Install Visual Studio Code<br>Once you have downloaded the\u00a0.deb package of Visual Studio Code, you can proceed with the installation process. Open a terminal window on your Nvidia Jetson Orin device and navigate to the directory where the\u00a0.deb package is\u00a0located.</p>\n<p>Use the following command to install the package:<br><br><strong>sudo dpkg -i package_file.deb</strong><br><br>Replace \u201cpackage_file.deb\u201d with the name of the\u00a0.deb package you downloaded. This command will install Visual Studio Code on your Nvidia Jetson Orin\u00a0device.</p>\n<p><strong>Step 3:</strong> Launch Visual Studio Code<br>After the installation process is complete, you can launch Visual Studio Code from the application menu or by typing \u201ccode\u201d in the terminal. Once launched, you\u2019ll have access to a powerful and feature-rich code editor that you can use for your software development projects.</p>\n<p><strong>Conclusion:</strong><br>In this tutorial, we\u2019ve covered the steps required to install Visual Studio Code on Nvidia Jetson Orin. By following these steps, you can set up a powerful development environment for your projects and take advantage of the features offered by Visual Studio\u00a0Code.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fefcd80a471e\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]}]}